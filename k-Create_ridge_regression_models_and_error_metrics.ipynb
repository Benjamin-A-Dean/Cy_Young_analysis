{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 26 - July - 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_DataFrame = pd.read_csv('../Data/AL_data.csv')\n",
    "NL_DataFrame = pd.read_csv('../Data/NL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Age', 'W', 'L', 'W-L%', 'W-L%_undefined', 'ERA', 'G',\n",
    "            'GS', 'GF', 'CG', 'SHO', 'SV', 'IP', 'H', 'R', 'ER', 'HR', 'BB', 'IBB',\n",
    "            'SO', 'HBP', 'BK', 'WP', 'BF', 'ERA+', 'FIP', 'WHIP', 'H9', 'HR9',\n",
    "            'BB9', 'SO9', 'SO/W', 'Season', 'Team_W', 'Team_L', 'Team_W-L%', 'GB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_training_data = AL_DataFrame[AL_DataFrame['Season'] < 2022]\n",
    "NL_training_data = NL_DataFrame[NL_DataFrame['Season'] < 2022]\n",
    "\n",
    "AL_testing_data = AL_DataFrame[AL_DataFrame['Season'] == 2022]\n",
    "NL_testing_data = NL_DataFrame[NL_DataFrame['Season'] == 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_regression_instance = Ridge(alpha=0.1)\n",
    "NL_regression_instance = Ridge(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL_regression_instance.fit(AL_training_data[features], AL_training_data['Share'])\n",
    "NL_regression_instance.fit(NL_training_data[features], NL_training_data['Share'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_cy_young_predictions = AL_regression_instance.predict(AL_testing_data[features])\n",
    "NL_cy_young_predictions = NL_regression_instance.predict(NL_testing_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_cy_young_predictions = pd.DataFrame(AL_cy_young_predictions, columns=['Predictions'], index=AL_testing_data.index)\n",
    "NL_cy_young_predictions = pd.DataFrame(NL_cy_young_predictions, columns=['Predictions'], index=NL_testing_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_cy_young_actual_and_predictions = pd.concat([AL_testing_data[['Name', 'Share']], AL_cy_young_predictions], axis=1)\n",
    "NL_cy_young_actual_and_predictions = pd.concat([NL_testing_data[['Name', 'Share']], NL_cy_young_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for the AL: 0.002674765484298228\n",
      "Mean Squared Error for the NL: 0.001931364665880398\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error for the AL: ' + str(mean_squared_error(AL_cy_young_actual_and_predictions['Share'],\n",
    "                                                                 AL_cy_young_actual_and_predictions['Predictions'])),\n",
    "      'Mean Squared Error for the NL: ' + str(mean_squared_error(NL_cy_young_actual_and_predictions['Share'],\n",
    "                                                                 NL_cy_young_actual_and_predictions['Predictions'])),\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_cy_young_actual_and_predictions = AL_cy_young_actual_and_predictions.sort_values('Share', ascending=False)\n",
    "NL_cy_young_actual_and_predictions = NL_cy_young_actual_and_predictions.sort_values('Share', ascending=False)\n",
    "\n",
    "AL_cy_young_actual_and_predictions['Rank'] = list(range(1,AL_cy_young_actual_and_predictions.shape[0]+1))\n",
    "NL_cy_young_actual_and_predictions['Rank'] = list(range(1,NL_cy_young_actual_and_predictions.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_cy_young_actual_and_predictions = AL_cy_young_actual_and_predictions.sort_values('Predictions', ascending=False)\n",
    "NL_cy_young_actual_and_predictions = NL_cy_young_actual_and_predictions.sort_values('Predictions', ascending=False)\n",
    "\n",
    "AL_cy_young_actual_and_predictions['Predicted_Rank'] = list(range(1,AL_cy_young_actual_and_predictions.shape[0]+1))\n",
    "NL_cy_young_actual_and_predictions['Predicted_Rank'] = list(range(1,NL_cy_young_actual_and_predictions.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_finder(rankings_DataFrame):\n",
    "    actual_top_5 = rankings_DataFrame.sort_values('Share', ascending=False).head(5)\n",
    "    predictions = rankings_DataFrame.sort_values('Predictions', ascending=False)\n",
    "    precisions = []\n",
    "    found = 0\n",
    "    checked = 1\n",
    "    for index, row in predictions.iterrows():\n",
    "        if row['Name'] in actual_top_5['Name'].values:\n",
    "            found += 1\n",
    "            precisions.append(found/checked)\n",
    "        checked += 1\n",
    "    return (sum(precisions)) / (len(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision for the AL: 0.7433333333333334\n",
      "Average precision for the NL: 0.6238095238095238\n"
     ]
    }
   ],
   "source": [
    "print('Average precision for the AL: ' + str(average_precision_finder(AL_cy_young_actual_and_predictions)),\n",
    "      'Average precision for the NL: ' + str(average_precision_finder(NL_cy_young_actual_and_predictions)), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = list(range(2003,2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions_AL = []\n",
    "all_predictions_AL = []\n",
    "for season in seasons[5:]:\n",
    "    training_data = AL_DataFrame[AL_DataFrame['Season'] < season]\n",
    "    testing_data = AL_DataFrame[AL_DataFrame['Season'] == season]\n",
    "    AL_regression_instance.fit(training_data[features], training_data['Share'])\n",
    "    predictions = AL_regression_instance.predict(testing_data[features]) \n",
    "    predictions = pd.DataFrame(predictions, columns=['Predictions'], index=testing_data.index)\n",
    "    actual_and_predictions = pd.concat([testing_data[['Name', 'Share']], predictions], axis=1)\n",
    "    all_predictions_AL.append(actual_and_predictions)\n",
    "    average_precisions_AL.append(average_precision_finder(actual_and_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions_NL = []\n",
    "all_predictions_NL = []\n",
    "for season in seasons[5:]:\n",
    "    training_data = NL_DataFrame[NL_DataFrame['Season'] < season]\n",
    "    testing_data = NL_DataFrame[NL_DataFrame['Season'] == season]\n",
    "    NL_regression_instance.fit(training_data[features], training_data['Share'])\n",
    "    predictions = NL_regression_instance.predict(testing_data[features]) \n",
    "    predictions = pd.DataFrame(predictions, columns=['Predictions'], index=testing_data.index)\n",
    "    actual_and_predictions = pd.concat([testing_data[['Name', 'Share']], predictions], axis=1)\n",
    "    all_predictions_NL.append(actual_and_predictions)\n",
    "    average_precisions_NL.append(average_precision_finder(actual_and_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_average_ap = (sum(average_precisions_AL)) / (len(average_precisions_AL))\n",
    "NL_average_ap = (sum(average_precisions_NL)) / (len(average_precisions_NL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average average precision for the AL vs 2022 average precision: 0.7781341602247853 vs 0.7433333333333334\n",
      "Average average precision for the NL vs 2022 average precision: 0.7309073099392792 vs 0.6238095238095238\n"
     ]
    }
   ],
   "source": [
    "print('Average average precision for the AL vs 2022 average precision: ' + str(AL_average_ap) + ' vs '\n",
    "                                              + str(average_precision_finder(AL_cy_young_actual_and_predictions)),\n",
    "      'Average average precision for the NL vs 2022 average precision: ' + str(NL_average_ap) + ' vs '\n",
    "                                              + str(average_precision_finder(NL_cy_young_actual_and_predictions)), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_difference_calculator(share_DataFrame):\n",
    "    share_DataFrame = share_DataFrame.sort_values('Share', ascending=False)\n",
    "    share_DataFrame['Rank'] = list(range(1,share_DataFrame.shape[0]+1))\n",
    "    share_DataFrame = share_DataFrame.sort_values('Predictions', ascending=False)\n",
    "    share_DataFrame['Predicted_Rank'] = list(range(1,share_DataFrame.shape[0]+1))\n",
    "    share_DataFrame['Rank_Difference'] = share_DataFrame['Rank'] - share_DataFrame['Predicted_Rank']\n",
    "    return share_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(league_stats, model_instance, seasons, features):    \n",
    "    average_precisions = []\n",
    "    all_predictions = []\n",
    "    for season in seasons:\n",
    "        training_data = league_stats[league_stats['Season'] < season]\n",
    "        testing_data = league_stats[league_stats['Season'] == season]\n",
    "        model_instance.fit(training_data[features], training_data['Share'])\n",
    "        predictions = model_instance.predict(testing_data[features]) \n",
    "        predictions = pd.DataFrame(predictions, columns=['Predictions'], index=testing_data.index)\n",
    "        actual_and_predictions = pd.concat([testing_data[['Name', 'Share']], predictions], axis=1)\n",
    "        actual_and_predictions = ranking_difference_calculator(actual_and_predictions)\n",
    "        all_predictions.append(actual_and_predictions)\n",
    "        average_precisions.append(average_precision_finder(actual_and_predictions))\n",
    "    return ((sum(average_precisions))/(len(average_precisions)), average_precisions, \n",
    "             pd.concat(all_predictions), model_instance.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_instance = Ridge(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_mean_average_precision, AL_average_precisions, AL_all_predictions, AL_coefficients = backtest(AL_DataFrame,\n",
    "                                                                                                 regression_instance,\n",
    "                                                                                                 seasons[5:], features)\n",
    "NL_mean_average_precision, NL_average_precisions, NL_all_predictions, NL_coefficients = backtest(NL_DataFrame,\n",
    "                                                                                                 regression_instance,\n",
    "                                                                                                 seasons[5:], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_regression_coefficients = pd.concat([pd.Series(AL_coefficients), pd.Series(features)], axis=1)\n",
    "NL_regression_coefficients = pd.concat([pd.Series(NL_coefficients), pd.Series(features)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_regression_coefficients = AL_regression_coefficients.sort_values(0, ascending=False)\n",
    "NL_regression_coefficients = NL_regression_coefficients.sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_ratios = (AL_DataFrame[['W', 'ERA', 'SO', 'IP', 'Season']].groupby('Season')\n",
    "                                                            .apply(lambda group: group/group.mean()))\n",
    "NL_ratios = (NL_DataFrame[['W', 'ERA', 'SO', 'IP', 'Season']].groupby('Season')\n",
    "                                                            .apply(lambda group: group/group.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_columns = [column + '_ratios' for column in ['W', 'ERA', 'SO', 'IP']]\n",
    "AL_DataFrame[ratio_columns] = AL_ratios[['W', 'ERA', 'SO', 'IP']]\n",
    "NL_DataFrame[ratio_columns] = NL_ratios[['W', 'ERA', 'SO', 'IP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += ratio_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_mean_average_precision_upd, AL_average_precisions_upd, AL_all_predictions_upd, AL_coefficients_upd = backtest(AL_DataFrame,\n",
    "                                                                                                        regression_instance,\n",
    "                                                                                                        seasons[5:], features)\n",
    "NL_mean_average_precision_upd, NL_average_precisions_upd, NL_all_predictions_upd, NL_coefficients_upd = backtest(NL_DataFrame,\n",
    "                                                                                                        regression_instance,\n",
    "                                                                                                        seasons[5:], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average average precision for the AL --> Original: 0.7781341602247853 --> Updated 0.7783606572402303\n",
      "Average average precision for the NL --> Original: 0.7309073099392792 --> Updated 0.7110870364950906\n"
     ]
    }
   ],
   "source": [
    "print('Average average precision for the AL --> Original: ' + str(AL_mean_average_precision) +\n",
    "                                          ' --> Updated ' + str(AL_mean_average_precision_upd),\n",
    "      'Average average precision for the NL --> Original: ' + str(NL_mean_average_precision) +\n",
    "                                          ' --> Updated ' + str(NL_mean_average_precision_upd), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_data = [['American League', AL_mean_average_precision, AL_mean_average_precision_upd],\n",
    "                          ['National League', NL_mean_average_precision, NL_mean_average_precision_upd]]\n",
    "average_precision_DataFrame = pd.DataFrame(average_precision_data, columns=['League', 'Ridge_regression',\n",
    "                                                                                      'Ridge_regression_upd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_DataFrame.to_csv('../Data/AL_data_updated.csv', index=False)\n",
    "NL_DataFrame.to_csv('../Data/NL_data_updated.csv', index=False)\n",
    "\n",
    "average_precision_DataFrame.to_csv('../Data/error_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
